{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "from tensorflow.keras import models, layers\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import os\n",
                "\n",
                "print(f\"TensorFlow version: {tf.__version__}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# CONFIGURATION\n",
                "BATCH_SIZE = 32\n",
                "IMAGE_SIZE = 256\n",
                "CHANNELS = 3\n",
                "EPOCHS = 20\n",
                "\n",
                "# Please update this path to point to your actual dataset directory\n",
                "# Example: \"./new plant diseases dataset(augmented)/New Plant Diseases Dataset(Augmented)/train\"\n",
                "dataset_dir = \"./dataset\" \n",
                "\n",
                "# Check if directory exists to avoid errors immediately\n",
                "if not os.path.exists(dataset_dir):\n",
                "    print(f\"WARNING: Dataset directory '{dataset_dir}' not found. Please set `dataset_dir` to your image folder.\")\n",
                "    # Creating a dummy dataset for demonstration if it doesn't exist\n",
                "    print(\"Creating dummy data structure for demonstration...\")\n",
                "    os.makedirs(f\"{dataset_dir}/healthy\", exist_ok=True)\n",
                "    os.makedirs(f\"{dataset_dir}/diseased\", exist_ok=True)\n",
                "    # Generate dummy images\n",
                "    import cv2\n",
                "    for i in range(5):\n",
                "        cv2.imwrite(f\"{dataset_dir}/healthy/img_{i}.jpg\", np.zeros((256,256,3), dtype=np.uint8))\n",
                "        cv2.imwrite(f\"{dataset_dir}/diseased/img_{i}.jpg\", np.ones((256,256,3), dtype=np.uint8)*255)\n",
                "    print(\"Dummy data created. REPLACE THIS WITH REAL DATA WITHOUT FAIL.\")\n",
                "\n",
                "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
                "    dataset_dir,\n",
                "    shuffle=True,\n",
                "    image_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
                "    batch_size=BATCH_SIZE\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Exploration and Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class_names = dataset.class_names\n",
                "print(\"Class names:\", class_names)\n",
                "\n",
                "plt.figure(figsize=(10, 10))\n",
                "for image_batch, labels_batch in dataset.take(1):\n",
                "    for i in range(12):\n",
                "        ax = plt.subplot(3, 4, i + 1)\n",
                "        plt.imshow(image_batch[i].numpy().astype(\"uint8\"))\n",
                "        plt.title(class_names[labels_batch[i]])\n",
                "        plt.axis(\"off\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Split Dataset\n",
                "80% Training, 10% Validation, 10% Test"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_dataset_partitions_tf(ds, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
                "    ds_size = len(ds)\n",
                "    \n",
                "    if shuffle:\n",
                "        ds = ds.shuffle(shuffle_size, seed=12)\n",
                "    \n",
                "    train_size = int(train_split * ds_size)\n",
                "    val_size = int(val_split * ds_size)\n",
                "    \n",
                "    train_ds = ds.take(train_size)\n",
                "    val_ds = ds.skip(train_size).take(val_size)\n",
                "    test_ds = ds.skip(train_size).skip(val_size)\n",
                "    \n",
                "    return train_ds, val_ds, test_ds\n",
                "\n",
                "train_ds, val_ds, test_ds = get_dataset_partitions_tf(dataset)\n",
                "\n",
                "# Cache and Prefetch for performance\n",
                "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
                "val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)\n",
                "test_ds = test_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preprocessing Layer: Rescaling and Resizing\n",
                "resize_and_rescale = tf.keras.Sequential([\n",
                "  layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
                "  layers.Rescaling(1./255)\n",
                "])\n",
                "\n",
                "# Data Augmentation Layer\n",
                "data_augmentation = tf.keras.Sequential([\n",
                "  layers.RandomFlip(\"horizontal_and_vertical\"),\n",
                "  layers.RandomRotation(0.2),\n",
                "])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model Building (CNN)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "input_shape = (BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
                "n_classes = len(class_names)\n",
                "\n",
                "model = models.Sequential([\n",
                "    resize_and_rescale,\n",
                "    data_augmentation,\n",
                "    layers.Conv2D(32, kernel_size = (3,3), activation='relu', input_shape=input_shape),\n",
                "    layers.MaxPooling2D((2, 2)),\n",
                "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
                "    layers.MaxPooling2D((2, 2)),\n",
                "    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),\n",
                "    layers.MaxPooling2D((2, 2)),\n",
                "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
                "    layers.MaxPooling2D((2, 2)),\n",
                "    layers.Flatten(),\n",
                "    layers.Dense(64, activation='relu'),\n",
                "    layers.Dense(n_classes, activation='softmax'),\n",
                "])\n",
                "\n",
                "model.build(input_shape=input_shape)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model.compile(\n",
                "    optimizer='adam',\n",
                "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
                "    metrics=['accuracy']\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "history = model.fit(\n",
                "    train_ds,\n",
                "    epochs=EPOCHS,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    verbose=1,\n",
                "    validation_data=val_ds\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Evaluation & Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "scores = model.evaluate(test_ds)\n",
                "print(\"Test Loss: \", scores[0])\n",
                "print(\"Test Accuracy: \", scores[1])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "acc = history.history['accuracy']\n",
                "val_acc = history.history['val_accuracy']\n",
                "\n",
                "loss = history.history['loss']\n",
                "val_loss = history.history['val_loss']\n",
                "\n",
                "plt.figure(figsize=(8, 8))\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(range(EPOCHS), acc, label='Training Accuracy')\n",
                "plt.plot(range(EPOCHS), val_acc, label='Validation Accuracy')\n",
                "plt.legend(loc='lower right')\n",
                "plt.title('Training and Validation Accuracy')\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(range(EPOCHS), loss, label='Training Loss')\n",
                "plt.plot(range(EPOCHS), val_loss, label='Validation Loss')\n",
                "plt.legend(loc='upper right')\n",
                "plt.title('Training and Validation Loss')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Prediction on New Images"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict(model, img):\n",
                "    img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())\n",
                "    img_array = tf.expand_dims(img_array, 0)\n",
                "\n",
                "    predictions = model.predict(img_array)\n",
                "\n",
                "    predicted_class = class_names[np.argmax(predictions[0])]\n",
                "    confidence = round(100 * (np.max(predictions[0])), 2)\n",
                "    return predicted_class, confidence\n",
                "\n",
                "plt.figure(figsize=(15, 15))\n",
                "for images, labels in test_ds.take(1):\n",
                "    for i in range(9):\n",
                "        ax = plt.subplot(3, 3, i + 1)\n",
                "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
                "        \n",
                "        predicted_class, confidence = predict(model, images[i].numpy())\n",
                "        actual_class = class_names[labels[i]] \n",
                "        \n",
                "        plt.title(f\"Actual: {actual_class},\\n Predicted: {predicted_class}.\\n Confidence: {confidence}%\")\n",
                "        plt.axis(\"off\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save Model\n",
                "model.save(f\"./models/crop_disease_model.h5\")\n",
                "print(\"Model saved successfully.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
